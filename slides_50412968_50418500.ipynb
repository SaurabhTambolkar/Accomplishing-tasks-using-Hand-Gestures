{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2966fa56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> CSE 676 DEEP LEARNING </center>\n",
    "## <center> PROJECT </center>\n",
    "## <center> Accomplishing Common Tasks Using Gestures </center>\n",
    "## <center> Saurabh Tambolkar | 50412968 | stambolk@buffalo.edu </center>\n",
    "## <center> Shantanu Kumar | 50418500 | skumar39@buffalo.edu </center>\n",
    "### <center> Project Code https://drive.google.com/drive/folders/18Gj9oBooDkMAudSsrlXTbW77ICse_wrV?usp=share_link </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be32a6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> INTRODUCTION </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d371dae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* In the recent years, the most predominant form of communication between the machine and the user is direct contact. This communication channel is based on devices such as a remote control, mouse, keyboard, touchscreen and other direct contact methods. \n",
    "* In the growing field of computer vision, gesture recognition is also one of the most important and growing problems and has been in many fields, such as human computer interaction, virtual reality systems, and sign language recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c0afe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* A hand gesture based interface provides much higher flexibility while also being user friendly because the user has the freedom of operating a machine using only his hand in front of his/her camera.\n",
    "* Several applications that employ static hand gesture systems are sign language processors, automated television control devices, smart home interaction controls, controlling a software interface, playing games using consoles (Nintendo Wii) & controlling virtual environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef04b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* More natural and intuitive methods, for example, sound and physical movements are generally used to accomplish human to human interaction. \n",
    "* Countless researchers are drawn to the flexibility and effeciency of these non-contact communication methods and have made them consider employing them to support human computer interaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a0228",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* A gesture can be defined as the motion of the body that intends to communicate with other agents in the environment. \n",
    "* To be successful at communicating via gestures, the sender and receiver must have mutual agreement on what constitues as a gesture first, otherwise there is a large scope of overall ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab409c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Gestures are the primary tool of symbolic communication and the natural form in which humans express themselves much more effectively. They can vary from simple to very complex actions which allow us to have a broad range of expressions. \n",
    "* Although communication using gestures can be achieved using many means, our hands remain the foremost instrument of choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e6f0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Gestures may be classified as dynamic or static.\n",
    "* A dynamic gesture generally varies over a peroid of time meanwhile static gestures, as the name suggests, tend to remain almost unchanged over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2e5e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Static gesture recognition methods have significant shortcomings and limitations. They can only recognize a single shape of the hand but struggle considerably with its spatial and state variations. \n",
    "* As an example, it can recognize whether the hand is in a state of \"unfolded\" or \"held\" state, but it cannot interpret if the state changes from one state to the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf709215",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Dynamic gesture recognition considers the spatial and temporal information of the whole procedure and recognizes the process of change of the target object, which in turn has serious research implications. \n",
    "* Before the fast development of deep learning, all the research on dynamic gesures relied mainly on manual extraction of feautres and then proceeded to building a sequence model for recognition. The downside to this approach was that the accuracy of this approach was fairly low and ineffecient, thus the commonly used methods now heavily rely on deep learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427aef6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Gesture recognition can be defined as the ability of a machine, generally a computer to understand human gestures and perform certain commands based on those gestures.\n",
    "* The main goal here is to develop a system that can identify and understand specific gestures and use them in a system to increase productivity and ease of use.\n",
    "* In this project, we employ static gestures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed69d7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* There were a lot of limitations in the initial research in image based gesture recognition due to low accuracy, poor real time recognition and high level of algorithmic run time complexity.\n",
    "* With the passage of time and advancement in the field of artifical intelligence, specially deep learning, and progress made in hardware which lead to faster computers, these issues were addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d98a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Historically, wearable data gloves were regularly used to capture the angles and positions of each joint in the user's gesture.\n",
    "* The difficulty and cost of a wearable sensor have restricted the widespread use of such a method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdbe71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Advances in computer vision technology have allowed the use of biological characteristics of human beings and brought about a shift in human computer interaction from traditional ways to new methods. \n",
    "* Our hands are the most flexible part of our body and thus, hand gestures can express rich and multiple forms of communication and thus, are widely used for communication between humans and devices smartphones, automobile infotainment systems, robotics. Gestures will replace touch and wire controlled input devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4704b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Hand tracking is vital to this proposed system and it undertakes various computer vision operations including hand segmentation, detection and tracking. \n",
    "* This traditional approach has high recognition accuracy but is costly and poorly scalable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b717f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Different sensors have different sensing capabilities. Mostly, a single sensor is used for gesture based interaction. \n",
    "* In order to recognize a gesture, raw data needs to be collected by the sensors. \n",
    "* There can be various ways to getting this data, for example, using a contactless sensor such as radar for hand movement detection or a wearable sensor such as a glove, which can measure the pressure applied by the fingers around the wrist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3107735",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Image based approaches mimic the use of eyes to recognise objects in this world. \n",
    "* Human machine interaction or robots need cameras to see and identify things. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f8e22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Deep Learning (DL) is an emerging field and is a sub category of machine learning inspired by the function of the human brain and its structure. It employs multiple hidden neural network layers for better learning of a model. \n",
    "* The Convolutional-Neural-Network (CNN) is a very famous DL model which is used in image-based applications. Nowadays, DL is used in visual object detection and recognition, speech recognition and many other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5170d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Multi-column deep CNNs that employ multiple parallel networks have been shown to improve recognition rates of single networks by 30-80 percent for various image classification task.\n",
    "* In recent years, many hand gesture recognition methods have been proposed using deep learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bed052",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "<center><img src = \"./images/1.1.png\">Hand Gesture Recognition<img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc341d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Hand gesture recognition process is divided into four steps; image acquisition, image enhancement, hand detection, feature extraction and finally gesture classification.\n",
    "* Input images are collected in the form of multiple frames for dynamic gestures, whereas for static gestures, a single image can also be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090bd4d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Image enhancement techniques are applied to increase the quality of input images. To apply deep learning based gesture recognition, the dataset needs to be large; therefore to enrich the input dataset, data augmentation is employed in this work using scaling, translating, rotating and shearing techniques.\n",
    "* Dynamic gesture recognition falls under the category of video classification since the dataset is mostly available in the form of video frames. Hence, both spatial and temporal domains features are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80804dec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* Dynamic gesture recognition is a difficult task because the images obtained from video recordings does not have consistent pixels; the camera is not fixed at one position and every person performs the same gesture in a different way.\n",
    "* Gesture includes different background with hand and arm continuous movement due to which it is not easy for an algorithm to predict the gestures with very high accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b753eee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "The project has multiple files, each of which has a specific function.\n",
    "1. PalmTracker.py Used to generate a custom dataset.\n",
    "2. ResizeImages.py To resize images.\n",
    "3. ModelTrainer.ipynb Trains the model.\n",
    "4. ContinuousGesturePredictor.py Running this file opens your webcam and takes continuous frames of your hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bbe41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>DATASET</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54eec4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dataset (Continued)</center>\n",
    "* We have utilized the camera provided in our PCs to create our dataset. \n",
    "* We used the images of our hands to train the model and tested it using the hand of others to account for the change in anatomy. \n",
    "* We also captured images in varying environment lighting to account for real world usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4faf6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dataset (Continued)</center>\n",
    "<img src = \"./images/2.0.1.png\"><center>Dataset Creation</center><img>\n",
    "* When we run the dataset generation script, it captures images of our hands when it enters the box shown in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6330e075",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Dataset (Continued)</center>\n",
    "* The project had the requirements given below. \n",
    "<ol>\n",
    " <li>Python 3\n",
    " <li> Tensorflow\n",
    " <li> OpenCV\n",
    " <li> TFLearn\n",
    " <li> NumPy\n",
    " <li> PIL Pillow\n",
    " <li> imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a3494e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dataset (Continued)</center>\n",
    "* When we run the dataset generation script once, it takes 100 images of a particular gesture using the running average by making use of OpenCV.\n",
    "* We have trained each gesture using a total of 1000 images. \n",
    "* We train a total of 5 gestures.\n",
    "* Effectively, we used 5,000 images to train our model.\n",
    "* It goes without saying that the model can be extended to more than 5 gestures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242a088",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dataset (Continued)</center>\n",
    "* After capturing the images, we resize all of the them to a common size of 28x28.\n",
    "* Resizing all images to the same size ensures our model learns appropriately. \n",
    "* We then convert them all into grayscale for uniformity.\n",
    "* We noticed that the background noise in images decreased overall accuracy significantly so we had to be extra careful while generating our test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeeac3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dataset (Continued)</center>\n",
    "* We also took the images for our testing dataset from varying distances to the camera.\n",
    "* We did this since in real world settings, we do not expect the user to make a gesture right in front of the camera.\n",
    "* An ideal neural network should be able to work at distances as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0f5ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dataset (Continued)</center>\n",
    "<img src = \"./images/g1.jpeg\"><center>Gesture 1 Fist</center><img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26646f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dataset (Continued)</center>\n",
    "<img src = \"./images/g2.jpeg\"><center>Gesture 2 Peace</center><img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec821e96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dataset (Continued)</center>\n",
    "<img src = \"./images/g3.jpeg\"><center>Gesture 3 Swag</center><img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e418ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dataset (Continued)</center>\n",
    "<img src = \"./images/g4.jpeg\"><center>Gesture 4 Swing</center><img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4ca4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Dataset (Continued)</center>\n",
    "<img src = \"./images/g5.jpeg\"><center>Gesture 5 Palm</center><img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be4dc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>RESULTS</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101be0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "* We have successfully demonstrated the application of our project by coming up with two scenarios. \n",
    "* In the first one, we control the volume of a song playing in the background using two gestures.\n",
    "* In the second one, a much more complex scenario, we control the cursor on our computer using hand gestures.\n",
    "* We have provided video demos for both the scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c4f49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "* We have mapped the volume up key to the gesture called \"Swing\".\n",
    "* On the other hand, we mapped the volume down key to the gesture called \"Swag\".\n",
    "* Please refer to the provided video demo links.\n",
    "* Both the gestures are visible in the provided demo.\n",
    "* <a href = \"https://www.youtube.com/watch?v=ZXuFn1XvlS8\">Demo Video</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7735d197",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "<img src = \"./images/2.1.png\"><center>Volume Up</center></img>\n",
    "* Here, we see that the gesture shown is used to increase volume of the music being played. \n",
    "* The achieved accuracy of the gesture is over 98%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89678d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "<img src = \"./images/2.2.png\"><center>Volume Down</center></img>\n",
    "* Here, we see that the gesture shown is used to decrease volume of the music being played. \n",
    "* The achieved accuracy of the gesture is over 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f43b4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "* We have mapped the move up command in the cursor to the gesture called \"Palm\".\n",
    "* We have mapped the move right command in the cursor to the gesture called \"Swag\".\n",
    "* We have mapped the move down command in the cursor to the gesture called \"Swing\".\n",
    "* We have mapped the move left command in the cursor to the gesture called \"Peace\".\n",
    "* We have mapped the click command in the cursor to the gesture called \"Fist\".\n",
    "* All 5 gestures are visible in the provided demo.\n",
    "* <a href = \"https://www.youtube.com/watch?v=bbpr_C2cSZw\">Demo Video 1</a>\n",
    "* <a href = \"https://www.youtube.com/watch?v=-K11-kjEKhE\">Demo Video 2</a>\n",
    "* Throughout the video, we see the cursor moving in accordance with the gestures explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10461d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "<img src = \"./images/2.3.png\"><center>Move Up</center></img>\n",
    "* Here, we see that the gesture shown is used to move the cursor in the upward direction.\n",
    "* The achieved accuracy of the gesture is over 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c93dc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "<img src = \"./images/2.4.png\"><center>Move Down</center></img>\n",
    "* Here, we see that the gesture shown is used to move the cursor in the downward direction.\n",
    "* The achieved accuracy of the gesture is over 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50dc0e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "<img src = \"./images/2.5.png\"><center>Mouse Click</center></img>\n",
    "* Here, we see that the gesture shown is used to cause a mouse click.\n",
    "* The achieved accuracy of the gesture is over 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f8741",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "<img src = \"./images/2.6.png\"><center>Move Right</center></img>\n",
    "* Here, we see that the gesture shown is used to move the cursor in the right direction.\n",
    "* The achieved accuracy of the gesture is over 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fe9c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "<img src = \"./images/2.7.png\"><center>Move Left</center></img>\n",
    "* Here, we see that the gesture shown is used to move the cursor in the left direction.\n",
    "* The achieved accuracy of the gesture is over 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750679d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Results (Continued)</center>\n",
    "* Here, we have successfully presented some examples of gestures being utilized to carry out common tasks. \n",
    "* The recent rapid development in the field of Deep Learning and Computer Vision have had a tremendous impact in making sure that such projects are possible.\n",
    "* Our model achieves a high level of accuracy which is instrumental to carrying out the required task successfully.\n",
    "* It is pretty obvious that the scope of this work is limitless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149258db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>STATE OF THE ART</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea317624",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Paper - Hand Gesture Recognition with 3D Convolutional Neural Networks</center>\n",
    "## <center>Author - Pavlo Molchanov, Shalini Gupta, Kihwan Kim, and Jan Kautz</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252628d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction</center>\n",
    "* In this paper, the authors introduce a hand gesture recognition system that utilizes depth and intensity channels with 3D convolutional neural networks.\n",
    "* Motivated by Molchanov et al, they interleave the two channels to build normalized spatio-temporal volumes, and train two separate sub-networks with these volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242420e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Introduction (Continued)</center>\n",
    "* To reduce potential over-fitting and improve generalization of the gesture classifier, the authors proposed an effective spatio-temporal data augmentation method to deform the input volumes of hand gestures.\n",
    "* The augmentation method also incorporates existing spatial augmentation techniques. This work bears similarities to the multi-sensor approach of Molchanov et al., but differs in the the use of two separate sub-networks and data augmentation.\n",
    "* The authors demonstrate that their system, with two sub-networks, that employs spatio-temporal data augmentation for training, outperforms both a single CNN and the baseline feature-based algorithm on the VIVA challenge’s dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73105359",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Dataset</center>\n",
    "* The VIVA challenge was organized to evaluate and advance the state-of-the-art in multi-modal dynamic hand gesture recognition under challenging conditions (with variable lighting and multiple subjects).\n",
    "* The VIVA challenge’s dataset contains 885 intensity and depth video sequences of 19 different dynamic hand gestures performed by 8 subjects inside a vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807378d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Dataset (Continued)</center>\n",
    "* Both channels were recorded with the Microsoft Kinect device and have a resolution of 115 × 250 pixels. The dataset was collected under varying illumination conditions. \n",
    "* The gestures were performed either with the right hand by subjects in the driver’s seat or with the left hand by subjects in the front passenger’s seat. The hand gestures involve hand and/or finger motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ffa23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training </center>\n",
    "* The process of training a CNN involves the optimization of the network’s parameters W to minimize a cost function for the dataset D. The authors selected negative log-likelihood as the cost function:\n",
    "<img src=\"./images/3.1.png\">\n",
    "* The authors performed optimization via stochastic gradient descent with mini-batches of 40 and 20 training samples for the LRN and the HRN, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4740db4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* The authors initialized the weights of the 3D convolution layers with random samples from a uniform distribution between [−Wb, Wb], where Wb = √6/(ni + no), and ni and no were the number of input and output neurons, respectively. They initialized the weights of the fully-connected hidden layers and the softmax layer with random samples from a normal distribution N (0, 0.01). The biases for all layers, except for the softmax layer, were initialized with a value of 1 in order to have a non-zero partial derivative. For the softmax layers biases were set to 0.\n",
    "* They trained the LRN and the HRN separately and merged them only during the forward propagation stage employed for decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf467933",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* They applied weight decay to all convolution layers. After processing each mini-batch we subtracted 0.5% from the network weights. \n",
    "* The authors observed that regularization with weight decay usually led to better generalization for gestures from different subjects. We also applied drop-out (with p = 0.5) to the outputs of the fully connected hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a845f8e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* During drop-out, the outputs were randomly (with p = 0.5) set to 0, and were consequently not used in the back-propagation step of that training iteration. For the forward propagation stage, the weights of the layer following the dropped layer were multiplied by 2 to compensate for the effect of drop-out.\n",
    "* For tuning the learning rate, they first initialized the rate to 0.005 and reduced it by a factor of 2 if the cost function did not improve by more than 10% in the preceding 40 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ce112",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* The authors terminated network training after the learning rate had decayed at least 4 times or if the number of epochs had exceeded 300.\n",
    "* Since the dataset is small, they did not reserve data from any subjects to construct a validation set. Instead, they selected the network configuration that resulted in the smallest error on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd3f5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Results </center>\n",
    "* They evaluated the performance of our dynamic hand gesture recognition system using leave-one-subject-out cross-validation on the VIVA challenge’s dataset\n",
    "* They used data from one of the 8 subjects for testing and trained the classifier with data from the 7 remaining subjects; then repeated this process for each of the 8 subjects and averaged the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7776ec4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* The authors applied various forms of regularization to the network in order to prevent overfitting even after a large number of training epochs. Data augmentation and drop-out were key components to successful generalization of the classifier.\n",
    "* They compared our classifier to the baseline method proposed by Ohn-Bar and Trivedi, which employs HOG+HOG2 features. Both the low and high resolution convolutional neural networks that they proposed, outperformed Ohn-Bar and Trivedi’s method by 9.8% and 5.5%, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a57f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* Furthermore, the final classifier that combined the outputs of LRN and HRN outperformed the baseline method by 13.0%. Moreover, 52% of the final classifier’s errors were associated with the second most probable class.\n",
    "* The results indicate that our CNN-based classifier for in-car dynamic hand gesture recognition considerably outperforms approaches that employ hand-crafted features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75b98d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* Observe that, individually, the depth data (accuracy = 65%) performed better than the intensity data (accuracy = 57%). On element-wise late multiplying the class-membership probabilities of the two LRNs trained individually with the two modalities, the accuracy improved to 70.4%.\n",
    "* However, the highest accuracy of 74.4% was obtained when the LRN was trained with interleaved depth and image gradient frames as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b9d9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* They observed that image gradients increased the final correct classification rate by 1.1%, and spatial and temporal elastic deformations applied to the training data increased it by 1.2% and 1.72%, respectively.\n",
    "* The classifier’s less confident decisions can be rejected by setting an empirical threshold. This helps to increase the correct classification rate, but at the cost of a greater number of missed gestures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a266d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Paper - Dynamic Hand Gesture Recognition Using 3D-CNN and LSTM Networks </center>\n",
    "## <center> Authors - Muneeb Ur Rehman, Fawad Ahmed, Muhammad Attique Khan, Usman Tariq, Faisal Abdulaziz Alfouzan, Nouf M. Alzahrani and Jawad Ahmad </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8082a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Introduction </center>\n",
    "* In this paper, a deep learning-based model which is combination of 3D-CNN and LSTM is proposed for recognizing dynamic hand gestures. To evaluate the proposed technique, the 20BN-jester dataset is used.\n",
    "* The 20BN-jester consists of 148,092 labeled video clips showing different people performing different dynamic hand gestures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad98d9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Introduction (Continued) </center>\n",
    "* The dataset has approximately 5000 video clips per class which are separated into training, validation and test sets. \n",
    "* Due to computational restrictions, only 15 classes have been used in this paper. As discussed in the later sections of the paper, the proposed model attained an accuracy of 97% on unseen data taken from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d19e71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Dataset </center>\n",
    "* Various video datasets are available publicly, however for this research, the 20BN-Jester, which is a very large-scale real-world dataset has been used. This dataset is generated by 1376 different actors in different unconstrained environments. It contains over 148,092 short video clips of 3s length.\n",
    "* Each video has 27 or more frames, which makes this the largest hand gesture dataset with more than 5 million frames in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faaf999",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Dataset (Continued) </center>\n",
    "* The original dataset has more than 4000 videos per class, however, 2000 random videos per class are chosen, which are further divided into 80% training and 20% validation set.\n",
    "* Video sequences of the 20BN-jester dataset have different length. For data preparation, the first step is to unify all the video-clips. Every video is limited to 30 frames per video. The dataset has videos of different length varying from 27 to 46 frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17980bc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training </center>\n",
    "* To address this problem, a new architecture which consists of a 3D-CNN followed by LSTM and a Softmax classifier is proposed in this paper as shown in the figure below.\n",
    "<img src = \"./images/3.2.png\"><center>Proposed Pipeline</center></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a48f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* Deep learning models need more data for improved training and subsequent performance. To achieve this, data augmentation techniques are used to modify the current dataset and create more variations of the images which will improve the model learning.\n",
    "* The features obtained from the 3D-CNN layers passes to the L2 batch normalization layer and are then fed to the LSTM layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b2a36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* This is followed by a dropout layer to avoid overfitting and finally the fully connected layer followed by output Softmax layer as shown in Fig. 6. L2 batch normalization is applied to obtain higher learning rates and to accelerate the initialization process for training and to reduce overfitting of the model.\n",
    "* Batch Normalization as shown by equation is carried out by using mean and variance of training data batches before the activation layer on the input.\n",
    "<img src = \"./images/3.2.1.png\">\n",
    "where, μ is the mean and σ is the standard deviation. These parameters are computed with respect to the batch size of the training data, ‘X’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4585a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* L2 regularization is a method used in deep learning with sum of square of scale weights which are added to the loss function as a penalty condition to be minimized as shown below. \n",
    "<img src = \"./images/3.2.2.png\">\n",
    "* L2 regularization ensures that the scale of weights should be close to zero. L2 regularization is also known as the “weight decay regularization”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17494cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* Each Conv3D has a kernel size (3 × 3 × 3), stride and pooling size (2 × 2 × 2) except for the first layer which is 1 × 2 × 2. This layer preserves the temporal details.\n",
    "* Feature maps have three different filter depths; 32, 64 and 128 which reduces the training parameters to approximately 3 million. Features are extracted by the 3D-CNN model and are then fed to the LSTM first layer with a unit size of 512. A dropout layer is added after the LSTM layer with a value 0.5 and then the probability results are computed using the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba304c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Result </center>\n",
    "* For the MobileNet-V2 + LSTM model, pre-trained weights of the ImageNet dataset were used which gave a validation accuracy of 84% at 20 epochs.\n",
    "* The results however did not improve further with validation loss not going below 0.25. The accuracy was reasonable but the real-time gestures prediction through a webcam was not accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5643557",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Result (Continued) </center>\n",
    "* After this, L2 batch normalization was introduced to MobilNet-V2+LSTM model and the accuracy improved to 87%, which was better but not acceptable as compared to other techniques proposed in the literature. \n",
    "* A light-weighted model consisting of 3D-CNN+LSTM with L2-batch normalization was used which had 3.7 million training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350245f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Result (Continued) </center>\n",
    "* The combination of two models and normalization technique for sequential video dataset produce competitive results as shown in table below.\n",
    "<img src = \"./images/3.2.3.png\">Accuracy, precision and recall using different models</img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec89831",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Result (Continued) </center>\n",
    "* Further, the authors have implemented our model with three optimizers: Adam, SGD (stochastic gradient descent) and Adadelta. \n",
    "* The Adam optimization technique achieved a validation accuracy of 95.2%, whereas the Stochastic Gradient Descent (SGD) optimizer achieved a lower accuracy. Adadelta optimizer with our proposed 3D-CNN+LSTM model achieved the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cab0c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Result (Continued) </center>\n",
    "* From a total 600 video-clips, 41 were classified as “Swiping Left” gesture. In actual, 40 video clips belong to swiping left class, hence the model predicted 40 clips correctly but 1 video clip was predicted false positive, therefore the recall is 98% for this class.\n",
    "* The model misclassified other classes as “Doing Other Things” class, therefore precision for this specific class is low but recall is very high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de92be71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Paper - Static Hand Gesture Recognition using Convolutional Neural Network with Data Augmentation </center>\n",
    "## <center> Authors - Md. Zahirul Islam, Mohammad Shahadat Hossain, Raihan Ul Islam </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369383eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Introduction </center>\n",
    "* This paper emphasizes on recognition of static hand gestures by building a model using CNN that can analyze large amount of image data and recognize static hand gestures.\n",
    "* The other objectives are to investigate the existing methods of gesture recognition and analyzing the effect of data augmentation in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e3f60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Dataset </center>\n",
    "* The authors selected 10 static gestures (Index, Peace, Three, Palm, Opened, Palm Closed, OK, Thumbs, Fist, Swing, Smile) to recognize. Each class has 800 images for training and 160 images for testing purpose. So total number of images is 8000 or training and 1600 for testing. Sample dataset is provided below.\n",
    "<center><img src = \"./images/3.3.1.png\">Sample Dataset</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451bc4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training </center>\n",
    "* A minimal pre-processing was applied over the dataset to reduce the computational complication and achieve better efficiency. Firstly, the background of the images was removed.\n",
    "* The background subtraction is mainly based on K-gaussian distribution which selects appropriate gaussian distribution for each pixel and provides a better adaptability on varying scenes due to illumination changes.After subtracting background, only the image of hand remains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f1c3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* Then the images were converted to grayscale image. Since grayscale images contain only one color channel it will be easier for CNN to learn.\n",
    "* Then a morphological erosion was applied. After that, median filter were applied to reduce the noise. In signal processing, it is often desirable to reduce noises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08be082",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* The figure below visualizes the pre-processing steps. The images were then resized to size 50x50 for feeding to CNN.\n",
    "<center><img src = \"./images/3.3.2.png\">Pre-processing Steps</img></center>\n",
    "* In addition to the self-developed dataset, another dataset named ”Hand Gesture Recognition Database” was also used in this experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780e809",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* The first convolution layer has 64 different filters with the kernel size 3x3. The activation function used in this layer is Rectified Linear Unit (ReLU). \n",
    "* ReLU was applied to introduce non-linearity and it has been proved that ReLU performs better than other activation functions such as tanh or sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b0bb38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* The input shape is 50x50x1 which means that gray-scale image of size 50x50 should be provided to this network. This layer produces the feature maps and passes them to the next layer.\n",
    "* Then the CNN has a max pooling layer with pool size 2x2 which takes the maximum value from a window of size 2x2. The spatial size of the representation is reduced progressively as the pooling layer takes only the maximum value and discards the rest. This layer helps the network to understand the images better because it only selects more important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3dd31b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* The next layer is another convolution layer and it has 64 different filters with the kernel size 3x3 and default stride. Again, ReLU was used as the activation function in this layer.\n",
    "* This layer is followed by another max pooling layer which has a pooling size 2x2. In this layer, first dropout was added which randomly discards 25% of the total neurons to prevent the model from over-fitting. Output from this layer is passed to the flatten layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a121f6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* Output from the previous layers are received by the flattening layer and they are flattened to a vector from two-dimensional matrix. This layer allows the fully connected layers to process the data achieved till now.\n",
    "* The next layer is first fully connected layer which has 256 nodes and ReLU was used as the activation function. The layer is followed by a dropout layer which excludes 25% of the neurons to prevent overfitting. The second fully connected layer again has 256 nodes to receive the vector produced by first fully connected layer and uses ReLU as activation layer. The layer is followed by a dropout layer to exclude 25% of the neurons to prevent over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c215dc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Results </center>\n",
    "* The experimental result shows that the model which was augmented with temporary data achieved 97.12% accuracy which is about 4% higher than the model without any augmented data.\n",
    "<center><img src = \"./images/3.3.3.png\"></center>\n",
    "* The graphs show that non-augmented model was early-stopped after 65 epochs and augmented model was early-stopped after 67 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce7681",
   "metadata": {},
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* The accuracy of augmented model was higher than non-augmented model at each epoch. It also shows that the progress of accuracy on augmented model was faster than non-augmented model.\n",
    "* The loss of augmented model was also less than non-augmented model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a7d2e",
   "metadata": {},
   "source": [
    "## <center> Results (Continued) </center>\n",
    "<center><img src = \"./images/3.3.4.png\"></center>\n",
    "* The diagonal values dictate the number of tuples that were correctly classified by the models and off-diagonal values represent the number of misclassified tuples. The higher the diagonal value, the better the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c03227",
   "metadata": {},
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* These matrices tell us that non-augmented model could not perform better on three classes (Index, Peace and Three) and the augmented model showed a better performance on these classes since the model was provided with increased amount of data. Thats why the performance was better in the case of augmented model.\n",
    "* The same experiment was performed again using train-test split 65-35. It was observed that accuracy for 65-35 split is 96.57%, which indicates the adaptability of the model for larger test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e1673",
   "metadata": {},
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* The same dataset was passed as input to Support Vector Machine (SVM) and K Nearest Neighbors (KNN) models. These models achieved accuracy of 72% and 75% respectively.\n",
    "* One of the reasons of poor performance showed by SVM and KNN is the adaptability issue with non-linear dataset. Since, the dataset was provided in a raw format, their accuracy were lower than CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e6fa6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Paper - Dynamic gesture recognition based on 2D convolutional neural network and feature fusion </center>\n",
    "## <center> Authors - Jimin Yu, Maowei Qin and Shangbo Zhou </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501c1b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Introduction </center>\n",
    "* To improve the problem of large network model parameters and training difficulties, the authors propose a strategy based on dual-channel 2D CNN and feature fusion.\n",
    "* First, the optical flow frames of the video data were extracted using the fractional Horn and Schunck (HS) optical flow method, and then five original key frames and optical flow key frames were extracted separately using an improved clustering algorithm and subjected to a horizontal stitching operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd718d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Introduction (Continued) </center>\n",
    "* The stitched original keyframe feature map is used to represent the spatial features in the video data, and the optical flow keyframe stitching map represents the temporal features in the video data. This method not only preserves the spatial and temporal features of the video, but it also greatly reduces the size of the dataset and improves the training efficiency. \n",
    "* Most current algorithms on dynamic gesture recognition using 2D CNN serialize the video datasets as a chart or a single image, which loses the information on the variation of key spatio-temporal features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890756a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Dataset </center>\n",
    "* To verify the effectiveness of the proposed method, the authors conducted related experiments on two public datasets (Cambridge Hand Gesture datasets and Northwestern University Hand Gesture datasets).\n",
    "* Cambridge Hand Gesture datasets contain a total of 9 gesture categories, consisting of 3 gesture shapes (flat, expand, V-shaped) and 3 basic actions (left, right, contract). There are 100 sets of data for each category, and the data is saved in the form of video clips, with a total of 900 video clips.\n",
    "* Northwestern University Hand Gesture datasets include 10 gesture categories, namely: move right, move left, rotate up, rotate down, move downright, move right-down, clockwise circle, counterclockwise, “Z” and cross. In each category, 15 persons participated in the collection and made 7 gestures (fist, hand, hold, index, side hand, side index, and thumb). There are 105 videos in each category, and there are a total of 1050 videos in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071bb2d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Dataset (Continued) </center>\n",
    "* Both of these two datasets have a certain degree of complexity and can comprehensively verify the pros and cons of the proposed model.\n",
    "* 60% of the database is used as the training set, 20% as the validation set, and 20% as the test set.\n",
    "* The distribution details of the datasets in the experiment are shown below.\n",
    "<center><img src = \"./images/3.4.1.png\">Dataset</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450fc6e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training </center>\n",
    "* When performing dynamic gesture recognition, in order to enable 2D CNN to analyze the spatial and temporal information of video data at the same time, the authors propose a fusion strategy.\n",
    "<center><img src = \"./images/3.4.2.png\">Fusion Strategy</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247ee70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* Firstly, they extract the original frames of the video. Then the fractional HS optical flow method is used to extract the optical flow frames corresponding to the original frame. Finally, the proposed clustering algorithm is used to extract original frames and optical flow frames as the keyframes of the video and carry out the horizontal splicing operation.\n",
    "* For a video data, its spatial dimension feature will be represented by the spliced original keyframe image, and time dimension feature be represented by the optical flow keyframe image. We use the feature fusion of the two kind keyframe images to represent the feature of the video data, and send it to the 2D recognition network for recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a9800",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* The structure of the recognition network is shown in the figure below. The spatial feature extraction network and the temporal feature extraction network have the same structure, and both are composed of three Squeeze-and-Excitation (SE) blocks.\n",
    "<center><img src = \"./images/3.4.3.png\">Recognition Network Structure</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed200d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* Since they have done a horizontal splicing operation on the keyframes, the length of the input picture is much greater than the width, so the convolutional layer of SE block 1_1 consists of 32 3×7 convolution kernels with a step size of 1. Enable the convolutional layer to extract more features in the lateral direction.\n",
    "* To enable the extracted features to better, reflect the global information of the feature map, they have added the SE module to allow the network to perform feature recalibration.\n",
    "* Through the SE module, the network can selectively emphasize useful global features and suppress less useful features. The convolutional layer of SE block 1_2 is composed of 64 3×5 convolution kernels with a step length of 1. The convolutional layer of SE block 1_3 consists of 128 3×3 convolution kernels with a step size of 1. Finally, after two full connections, spatio-temporal feature fusion is performed and the fused features are input to the full connection layer to realize the classification of gesture actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba0df9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "* The structure of the SE module is shown here.\n",
    "<center><img src = \"./images/3.4.4.png\">Squeeze & Excitation Module</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b830c5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Training (Continued) </center>\n",
    "<center><img src = \"./images/3.4.5.png\">Proposed Clustering Algorithm</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b5621",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* When analyzing the network performance, in order to determine how many keyframes work best when extracted for training. They chose to use 3, 4, 5, 6, 7 keyframes for the comparative analysis. \n",
    "<center><img src = \"./images/3.4.6.png\">Accuracy Curves</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bef9c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* When designing the network structure, in order to determine how many SE blocks can be used for each branch to get better recognition, they chose to use 1, 2, 3, 4, 5 SE blocks respectively for the experiment, and the results are shown below.\n",
    "training. We chose to use 3, 4, 5, 6, 7 keyframes for the comparative analysis. \n",
    "<center><img src = \"./images/3.4.7.png\">Accuracy Comparison of the Network Structure</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734709f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* It can be seen that when each branch is made up of 3 SE blocks, there is a great improvement compared to 1 and 2. However, when each branch is composed of 4 and 5 SE blocks, the improvement in accuracy is not significant and increases the parameters of the network. Therefore, we use the structure of 3 SE blocks per branch.\n",
    "* The accuracy of the proposed method is 97.6% on the Northwestern University datasets and 98.6% on the Cambridge datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9322b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Results (Continued) </center>\n",
    "* To compare the efficiency of the various algorithms more intuitively, the authors calculated the time taken by the model to classify a test sequence. The results are shown in Table 6. The time of the proposed algorithm is 9.93 s on the Northwestern University gesture dataset and 4.02 s on the Cambridge gesture dataset, both of which are more significant improvements over previous algorithms.\n",
    "<center><img src = \"./images/3.4.8.png\">Computation time for classifying a test sequence</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888428c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Paper - Real-Time Hand Gesture Recognition Using Fine-Tuned Convolutional Neural Network </center>\n",
    "## <center> Authors - Jaya Prakash Sahoo, Allam Jaya Prakash, Paweł Pławiak and Saunak Samantray </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81dfd84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Introduction </center>\n",
    "* An efficient and accurate hand gesture recognition model is highly essential for the recognition of hand gestures in real-time applications. To develop such a recognition model, a score-level fusion technique between two fine-tuned CNNs such as AlexNet and VGG-16 is proposed in this work.\n",
    "* An end-to-end fine-tuning of the deep CNNs such as AlexNet and VGG-16 is performed on the training gesture samples of the target dataset. Then, the score-level fusion technique is applied between the output scores of the fine-tuned deep CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce1212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Introduction (Continued) </center>\n",
    "* The performance of recognition accuracy is evaluated on two publicly available benchmark American Sign Language (ASL) large-gesture class datasets.\n",
    "* A real-time gesture recognition system is developed using the proposed technique and tested in subject-independent mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f17f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Dataset </center>\n",
    "* Several sensors are available to develop an HGR system.\n",
    "* A comparison among the sensors with their advantages and limitations is presented in the table below.\n",
    "<center><img src = \"./images/3.5.1.png\">Different Sensors</img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc19b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Dataset (Continued) </center>\n",
    "* The table shows that an HGR system developed using the data glove is more accurate and robust, but the user feels uncomfortable and their hand is restricted by wearing the glove.\n",
    "* Again for the leap motion sensor-based HGR, the tracking of the hand is completed with a high precision; however, the hand coverage area is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d0855",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Dataset (Continued) </center>\n",
    "* Compared to the other sensors, a vision-based sensor does not require any object to be put on the user’s hand, and the hand gestures are captured using the sensor with free hand.\n",
    "* This advantage of the sensor attracts researchers to develop the HGR system using the vision sensor. In this work, the Kinect V2 depth sensor is used to develop the proposed hand gesture recognition system as it is easier to segment the hand region more accurately from the image frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c726da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Training</center> \n",
    "* An overview of the proposed hand gesture recognition system is shown below.\n",
    "<center><img src = \"./images/3.5.2.png\">Proposed System</img></center>\n",
    "* As shown in the figure, the recognition of static hand gesture images is achieved by the following steps: data acquisition, pre-processing and recognition of hand gestures using proposed technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4fd1aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Training (Continued)</center> \n",
    "* The objective of this step is to segment the hand region from the hand gesture image frame and to resize it into the pre-trained CNN’s input image size. The color and depth map images are obtained from the Kinect depth camera as shown in the figure. \n",
    "<center><img src = \"./images/3.5.3.png\">Proposed System</img></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ecb72f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Training (Continued)</center> \n",
    "* Between both inputs, only the depth map image is considered for recognition of static hand gesture. Depth thresholding is used for segmentation of the hand region from the depth map.\n",
    "* An empirically determined value of 10 cm is chosen as a depth threshold value to segment the hand from the background. \n",
    "* The maximum-area-based filtering technique is used to find the hand region and remove the noise section of the segmented image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a868b56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Training (Continued)</center> \n",
    "* The maximum-area-based filtering technique is used to find the hand region and remove the noise section of the segmented image.\n",
    "* Following this, the bounding box region is cropped form the segmented image. Both pre-trained CNNs operate with three-channel input images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8d437",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Training (Continued)</center> \n",
    "* The cropped hand gesture images are normalized to generate a single-channel image in a range from [0,255] using the formula given below. \n",
    "<center><img src = \"./images/3.5.8.png\">Formula</img></center>\n",
    "where D denotes the depth values in the depth map image and (𝑥,𝑦) are the pixel indices in the depth map. max(𝐷) and min(𝐷) are the maximum and minimum depth vales in the depth map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc7d5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Results</center> \n",
    "* The mean accuracy of test gesture samples with the score fusion between the two fine-tuned CNNs is shown in the figure. \n",
    "<center><img src = \"./images/3.5.6.png\">Results</img></center>\n",
    "* The tabulation result shows that the proposed technique performs better in terms of mean accuracy (average ± standard deviation) compared to both fine-tuned CNNs on the above datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d81cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Results (Continued)</center> \n",
    "* The proposed technique performs at 90.26% and 56.18% mean accuracy with the LOO CV test on the MU dataset and HUST-ASL dataset, respectively.\n",
    "* Similarly, for the regular CV, test the performance of the proposed technique shows a mean accuracy of 98.14% and 64.55% for both datasets. The result shows that the performance of the LOO CV test is higher compared to the regular CV test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5d2bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Results (Continued)</center> \n",
    "* The reason for this is described below. The LOO CV technique is a user-independent CV technique.\n",
    "* In this technique, the performance of the trained model is evaluated using the gesture samples of the user, who does not take part in the model development.\n",
    "* However, in regular CV, the gesture samples of all the users in the dataset take part in the training and testing processes.\n",
    "* Hence, this CV test is user-biased. Therefore, the model performance using the regular CV test is higher than the LOO CV test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebc800",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Results (Continued)</center> \n",
    "<center><img src = \"./images/3.5.7.png\">The subject-wise comparison of recognition accuracy in the LOO CV test on both datasets used</img></center>\n",
    "* The most confusing gesture poses are ‘6’ and ‘w’ in the MU dataset. A total of 52.9% of gesture pose ‘6’ is misclassified to gesture pose ‘w’, and 48.6% of gesture pose ‘w’ is misclassified to gesture pose ’6’ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a80b15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Results (Continued)</center> \n",
    "* The performance of the proposed technique shows better results than the GB-ZM and GB-HU results.\n",
    "* Results shows that the proposed technique provides 5.85% and 8.01% higher mean accuracy than the earlier reported techniques on the LOO CV and regular CV tests, respectively. A fine-tuned CNN model is able to extract the feature from the hand gesture image accurately. This shows the effectiveness of fine-tuned CNNs.\n",
    "* This paper has introduced a score-level fusion technique between two fine-tunned CNNs for the recognition of vision-based static hand gestures. The proposed network eliminates the requirement of illumination variation, rotation and hand region segmentation as pre-processing steps for the color image MU dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed4666",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>References</center> \n",
    "1. P. Molchanov, S. Gupta, K. Kim and J. Kautz, \"Hand gesture recognition with 3D convolutional neural networks,\" 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2015, pp. 1-7, doi: 10.1109/CVPRW.2015.7301342.\n",
    "2. Hakim, N.L.; Shih, T.K.; Kasthuri Arachchi, S.P.; Aditya, W.; Chen, Y.-C.; Lin, C.-Y. Dynamic Hand Gesture Recognition Using 3DCNN and LSTM with FSM Context-Aware Model. Sensors 2019, 19, 5429. https://doi.org/10.3390/s19245429\n",
    "3. M. Z. Islam, M. S. Hossain, R. ul Islam and K. Andersson, \"Static Hand Gesture Recognition using Convolutional Neural Network with Data Augmentation,\" 2019 Joint 8th International Conference on Informatics, Electronics & Vision (ICIEV) and 2019 3rd International Conference on Imaging, Vision & Pattern Recognition (icIVPR), 2019, pp. 324-329, doi: 10.1109/ICIEV.2019.8858563.\n",
    "4. Yu J, Qin M, Zhou S. Dynamic gesture recognition based on 2D convolutional neural network and feature fusion. Sci Rep. 2022 Mar 14;12(1):4345. doi: 10.1038/s41598-022-08133-z. PMID: 35288612; PMCID: PMC8921226.\n",
    "5. Sahoo, J.P.; Prakash, A.J.; Pławiak, P.; Samantray, S. Real-Time Hand Gesture Recognition Using Fine-Tuned Convolutional Neural Network. Sensors 2022, 22, 706. https://doi.org/10.3390/s22030706 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2311eef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>END</center> "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
